from rogger.util.logging import logger
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from rogger.vectorizer.e5 import E5Retriever
from rogger.vectorizer.bm25 import BM25Retriever
from rogger.vectorizer.tfidf import TfIdfRetriever
from rogger.retriever.manual import create_vec_store
from rogger.util.globals import *
from rogger.util.config import *
import streamlit as st
import time
from langchain_community.llms.ollama import Ollama
from rogger.llm.aya import Aya101LLM

# from googletrans import Translator

st.set_page_config(layout="wide")


st.title("ğŸ¦œğŸ”— Rogger")
st.session_state.theme = "dark"

st.markdown(
    """
<style>
    stChatMessage{
        text-align: right;
        direction:rtl;

    }
    .st-emotion-cache-janbn0 {
        flex-direction: row-reverse;
    }
    .st-emotion-cache-4oy321{
        text-align: left;
    }
    .st-emotion-cache-vdokb0{
        text-align: right;
        direction:rtl;
    }

    textarea {
        font-size: 16px;
        text-align: right;
        direction:rtl;
    }
    p {
        text-align: right;
        direction:rtl;
    }
</style>
""",
    unsafe_allow_html=True,
)


def _save(name, buffer):
    with open(f"assets/test/{name}.json", "w", encoding="utf-8") as file:
        json.dump(buffer, file, ensure_ascii=False, indent=4)


def retrieve_page_content(retriever: BaseRetriever, query: str) -> List[Document]:
    buffer = []
    result = retriever.invoke(query)
    for doc in result:
        classname = retriever.__repr__()[:13]
        logger.info(f"appending from {classname} CLASS")
        buffer.append(Document(page_content=doc.page_content))
    _save(name="raw", buffer=[x.page_content for x in buffer])
    return buffer


if "e5" not in st.session_state:
    logger.info("Initiating E3Embeddings ...")
    st.session_state.e5 = E5Retriever
if "bm25" not in st.session_state:
    logger.info("Initiating Bm25Embeddings ...")
    st.session_state.bm25 = BM25Retriever
if "tfidf" not in st.session_state:
    logger.info("Initiating Bm25Embeddings ...")
    st.session_state.tfidf = TfIdfRetriever
if "chat_id" not in st.session_state:
    logger.info("Initiating ChatId ...")
    st.session_state.chat_id = None
if "docs_raw" not in st.session_state:
    logger.info("Initiating Docs ...")
    st.session_state.docs_raw = create_docs()
if "vecstore_bm25" not in st.session_state or "vecstore_e5" not in st.session_state:
    logger.info("Initiating retriever on page content...")
    st.session_state.vecstore_e5 = create_vec_store(
        retriever=st.session_state.e5, docs=st.session_state.docs_raw
    )
    st.session_state.vecstore_bm25 = create_vec_store(
        retriever=st.session_state.bm25, docs=st.session_state.docs_raw
    )
if "llm" not in st.session_state:
    logger.info("Initiating LLM ...")
    st.session_state.llm = Ollama(
        model="aya:35b-23", base_url="http://192.168.202.254:11434", temperature=0
    )
if "chat_history" not in st.session_state:
    logger.info("Initiating chat-history")
    st.session_state.chat_history = []


# if "reranker" not in st.session_state:
#     logger.info("Initiating reranker ...")
#     st.session_state.reranker = Reranker()


def click_button():
    st.session_state.clear()


def make_prompt(message: str = "", context: list = []):
    #     template = f"""Ø¨Ø§ Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø·Ø§Ù„Ø¨ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¨Ø®Ø´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´ÛŒÙ† Ù‚Ø³Øª Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø³ÙˆØ§Ù„ Ù…Ù† Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ù† Ùˆ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù…Ù† Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø³ÙˆØ§Ù„ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡
    # Ù…Ù† ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†Ù…. Ø§Ø² Ø§ÛŒÙ† Ø¬Ù‡Øª ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ù¾Ø§Ø³Ø® Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ú¯Ùˆ.

    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´ÛŒÙ†:
    # {context}

    # Ø³ÙˆØ§Ù„:
    # {message}"""

    template = f"""{message}

Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´ÛŒÙ†:
{context}

ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¨Ø®Ø´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´ÛŒÙ† Ø¬ÙˆØ§Ø¨ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ú¯Ùˆ Ùˆ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ´ÛŒÙ† Ù†ÛŒØ§Ù…Ø¯Ù‡ Ø¨ÙˆØ¯ Ø¨Ú¯Ùˆ Ù†Ù…ÛŒ Ø¯Ø§Ù†Ù….
"""

    logger.info(f"Invoking {template}")
    return template


st.button("Reset", type="primary", on_click=click_button)
for message in st.session_state.chat_history:
    if message["src"] == "Human":
        with st.chat_message("Human"):
            st.markdown(message["text"])
    elif message["src"] == "AI":
        with st.chat_message("AI"):
            st.markdown(message["text"])


def generate_response_llm(input_text, session):
    print(len(st.session_state.docs_raw))
    logger.info("Invoking prompt to LLM ...")
    raw_context = retrieve_page_content(st.session_state.vecstore_bm25, input_text)
    extend_context = retrieve_page_content(st.session_state.vecstore_e5, input_text)
    raw_context.extend(extend_context)
    ctx_list = list(set([x.page_content for x in raw_context]))
    _save(name="merged", buffer=ctx_list)
    context = "\n".join(ctx_list)
    query = make_prompt(input_text, context=context)
    logger.info(f"query LLM with {query}")
    response = st.session_state.llm.invoke(query)
    logger.warning(f"raw response from llm {response}")
    response.replace(":", "")
    for letter in response:
        time.sleep(0.01)
        yield letter
    session.append({"src": "AI", "text": response})


user_query = st.chat_input("Enter a prompt here")
if user_query is not None and user_query != "":
    st.session_state.chat_history.append({"src": "Human", "text": user_query})

    with st.chat_message("Human"):
        st.markdown(user_query, unsafe_allow_html=True)
    with st.chat_message("AI"):
        st.write_stream(
            generate_response_llm(user_query, st.session_state.chat_history)
        )
